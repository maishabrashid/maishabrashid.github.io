<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Maisha Binte Rashid | Portfolio</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #ffffff;
        }

        /* Navigation */
        nav {
            background: #2c3e50;
            color: white;
            padding: 1rem 0;
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }

        nav .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        nav h1 {
            font-size: 1.5rem;
        }

        nav ul {
            list-style: none;
            display: flex;
            gap: 2rem;
        }

        nav a {
            color: white;
            text-decoration: none;
            transition: opacity 0.3s;
        }

        nav a:hover {
            opacity: 0.7;
        }

        /* Container */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Sections */
        section {
            padding: 4rem 0;
            border-bottom: 1px solid #e0e0e0;
        }

        section:last-child {
            border-bottom: none;
        }

        section h2 {
            font-size: 2rem;
            margin-bottom: 2rem;
            color: #2c3e50;
        }

        /* About Section */
        .about-content {
            display: grid;
            grid-template-columns: 250px 1fr;
            gap: 3rem;
            align-items: start;
        }

        .profile-image {
            width: 100%;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .bio h3 {
            font-size: 1.8rem;
            margin-bottom: 0.5rem;
            color: #2c3e50;
        }

        .bio .title {
            font-size: 1.1rem;
            color: #7f8c8d;
            margin-bottom: 1rem;
        }

        .bio p {
            margin-bottom: 1rem;
            text-align: justify;
        }

        .social-links {
            margin-top: 1.5rem;
        }

        .social-links a {
            display: inline-block;
            margin-right: 1rem;
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
        }

        .social-links a:hover {
            text-decoration: underline;
        }

        .cv-button {
            background: #27ae60 !important;
            color: white !important;
            padding: 0.5rem 1rem !important;
            border-radius: 6px !important;
            text-decoration: none !important;
            display: inline-block !important;
            transition: background 0.3s !important;
        }

        .cv-button:hover {
            background: #229954 !important;
            text-decoration: none !important;
        }

        .research-interests {
            background: #f8f9fa;
            padding: 1rem;
            border-radius: 6px;
            margin-top: 1rem;
        }

        .research-interests strong {
            color: #2c3e50;
        }

        /* Publications Section */
        .publication {
            margin-bottom: 2.5rem;
            padding: 1.5rem;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #3498db;
        }

        .publication h3 {
            font-size: 1.2rem;
            margin-bottom: 0.5rem;
            color: #2c3e50;
            line-height: 1.4;
        }

        .publication .authors {
            color: #555;
            margin-bottom: 0.5rem;
        }

        .publication .venue {
            font-style: italic;
            color: #7f8c8d;
            margin-bottom: 0.5rem;
        }

        .publication .links a {
            display: inline-block;
            margin-right: 1rem;
            margin-top: 0.5rem;
            padding: 0.3rem 0.8rem;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9rem;
        }

        .publication .links a:hover {
            background: #2980b9;
        }

        /* Projects Section */
        .project {
            margin-bottom: 2rem;
            padding: 1.5rem;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #27ae60;
        }

        .project h3 {
            font-size: 1.3rem;
            margin-bottom: 0.5rem;
            color: #2c3e50;
        }

        .project p {
            color: #555;
            margin-bottom: 0.5rem;
        }

        .project ul {
            margin-left: 1.5rem;
            margin-top: 0.5rem;
        }

        .project li {
            margin-bottom: 0.3rem;
            color: #555;
        }

        .project-link {
            display: inline-block;
            margin-top: 0.5rem;
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
        }

        .project-link:hover {
            text-decoration: underline;
        }

        /* Experience Section */
        .experience-item {
            margin-bottom: 2.5rem;
            padding: 1.5rem;
            background: #f8f9fa;
            border-radius: 8px;
        }

        .experience-item h3 {
            font-size: 1.3rem;
            color: #2c3e50;
            margin-bottom: 0.3rem;
        }

        .experience-item .company {
            font-weight: 600;
            color: #555;
        }

        .experience-item .duration {
            color: #7f8c8d;
            font-style: italic;
            margin-bottom: 1rem;
        }

        .experience-item ul {
            margin-left: 1.5rem;
        }

        .experience-item li {
            margin-bottom: 0.5rem;
            color: #555;
        }

        /* News Section */
        .news-info {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }

        .news-info p {
            font-size: 1.1rem;
        }

        .news-info a {
            color: #3498db;
            text-decoration: none;
        }

        .news-info a:hover {
            text-decoration: underline;
        }

        /* Footer */
        footer {
            background: #2c3e50;
            color: white;
            text-align: center;
            padding: 2rem 0;
            margin-top: 4rem;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            nav ul {
                gap: 1rem;
                font-size: 0.9rem;
            }

            nav h1 {
                font-size: 1.2rem;
            }

            .about-content {
                grid-template-columns: 1fr;
                gap: 2rem;
            }

            .profile-image {
                max-width: 250px;
                margin: 0 auto;
            }

            section {
                padding: 2rem 0;
            }

            section h2 {
                font-size: 1.5rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav>
        <div class="container">
            <h1>Maisha Binte Rashid</h1>
            <ul>
                <li><a href="#about">About</a></li>
                <li><a href="#News">News</a></li>
                <li><a href="#research">Research</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#projects">Projects</a></li>
                
            </ul>
        </div>
    </nav>

    <!-- About Section -->
    <section id="about">
        <div class="container">
            <div class="about-content">
                <img src="pp.jpg" alt="Maisha Binte Rashid" class="profile-image">
                <div class="bio">
                    <h3><b>Maisha Binte Rashid</b></h3>
                    <p class="title">Ph.D. Candidate, Computer Science, Baylor University</p>
                    <p>
                        Welcome! My name is Maisha and I am a Ph.D. candidate in Computer Science at Baylor University, where I am advised by Pablo Rivas. I am part of the <a href="https://www.rivas.ai/" target="_blank">Rivas Lab</a>, specializing in multimodal machine learning 
                        and AI robustness. My research focuses on integrating text and image data to defend against adversarial attacks, 
                        with particular expertise in large-scale language models (LLMs), natural language processing, and deep learning.
                    </p>
                    <p>
                        As a Graduate Research Assistant, I have contributed to NSF-funded projects and enhanced AI safety for models 
                        like GPT-2 and Vision Transformer. My work includes developing the Adversarial Vulnerability Index (AVI), 
                        a novel metric for assessing risk in vision-language models through progressive stress testing.
                    </p>
                    <p>
                        Prior to my doctoral studies, I worked as a Programmer Analyst at Computer Ease Ltd. in Bangladesh, where I 
                        led development of B2B transaction systems and client dashboards, significantly improving processing efficiency 
                        and data fidelity.
                    </p>
                    <div class="research-interests">
                        <strong>Research Interests:</strong> Multimodal Machine Learning, AI Safety & Robustness, Adversarial Attacks & Defense, 
                        Vision-Language Models, Natural Language Processing, Deep Learning
                    </div>
                    <div class="social-links">
                        <a href="mailto:maishabrashid05@gmail.com">ðŸ“§ Email</a>
                        <a href="https://www.linkedin.com/in/maisha-binte-rashid" target="_blank">ðŸ’¼ LinkedIn</a>
                        <a href="https://github.com/maishabrashid" target="_blank">ðŸ’» GitHub</a>
                        <a href="ResumeMaishaBinteRashid.pdf" target="_blank" class="cv-button">ðŸ“„ View CV</a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- News Section -->
    <section id="news">
        <div class="container">
            <h2>News</h2>
            <div class="news-info">
                <p>
                <strong>Presenting at WiML@NeurIPS 2025:</strong>
                <p>
                I'll be presenting "<em>Understanding Adversarial Weakness in Vision-Language Models</em>" 
                at WiML Workshop @ NeurIPS 2025 in San Diego, California 
                (Dec 2 2025, 
                <a href="https://openreview.net/forum?id=xQQ87B2I3j" target="_blank">
                    OpenReview
                </a>).
                </p>
            </p>
            </div>
        </div>
    </section>

    <!-- Research/Experience Section -->
    <section id="research">
        <div class="container">
            <h2>Research & Experience</h2>
            
            <div class="experience-item">
                <h3>Graduate Research Assistant</h3>
                <p class="company">Baylor University, Waco, TX</p>
                <p class="duration">August 2021 â€“ Present</p>
                <ul>
                    <li>Enhanced AI safety for models like GPT-2 and Vision Transformer, defending against adversarial attacks with integrated image and text data, boosting accuracy by 12%</li>
                    <li>Led multimodal deep learning project focusing on image and text data analysis for NSF-funded research</li>
                    <li>Developed the Adversarial Vulnerability Index (AVI) metric for vision-language models, using progressive stress testing from random noise to gradient-based attacks</li>
                    <li>Utilized Flamingo model to identify stolen car parts from comprehensive 400 GB dataset of Craigslist images and posts</li>
                    <li>Directed student engagement in graduate Natural Language Processing (NLP) course, advancing competencies in machine learning models and text analysis</li>
                </ul>
            </div>

            <div class="experience-item">
                <h3>Programmer Analyst</h3>
                <p class="company">Computer Ease Ltd., Dhaka, Bangladesh</p>
                <p class="duration">January 2020 â€“ July 2021</p>
                <ul>
                    <li>Led development of 20 modules across 4 websites for B2B transactions using .NET and C#, reducing processing time by 30%</li>
                    <li>Developed two client dashboards for daily transaction monitoring using Angular and .NET Core, dropping error rates by 40%</li>
                    <li>Enhanced client operations and data fidelity through optimized system architecture</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Publications Section -->
    <section id="publications">
        <div class="container">
            <h2>Publications</h2>

            <div class="publication">
                <h3>A Framework for Evaluating Vision-Language Model Safety: Building Trust in AI for Public Sector Applications</h3>
                <p class="authors">M. Rashid, P. Rivas</p>
                <p class="venue">AAAI 2025 Workshop on AI for Social Impact: Bridging Innovations in Finance, Social Media, and Crime Prevention</p>
                <div class="links">
                    <a href="https://arxiv.org/pdf/2502.16361" target="_blank">Paper</a>
                </div>
            </div>
            
            <div class="publication">
                <h3>AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning</h3>
                <p class="authors">M. Rashid, P. Rivas</p>
                <p class="venue">In Proceedings of ACM KDD 2024 (KDD â€™24). ACM, New York, NY, USA</p>
                <div class="links">
                    <a href="https://arxiv.org/pdf/2407.21174" target="_blank">Paper</a>
                </div>
            </div>

            <div class="publication">
                <h3>Leveraging OpenFlamingo for Multimodal Embedding Analysis of C2C Car Parts Data</h3>
                <p class="authors">M. Rashid, P. Rivas</p>
                <p class="venue">World Congress in Computer Science, Computer Engineering & Applied Computing (Springer Nature Switzerland), July 22, 2024</p>
                <div class="links">
                    <a href="https://arxiv.org/pdf/2503.17408" target="_blank">Paper</a>
                </div>
            </div>

            <div class="publication">
                <h3>Navigating the multimodal landscape: A review on integration of text and image data in machine learning architectures</h3>
                <p class="authors">M. Rashid, MS Rahaman, P. Rivas</p>
                <p class="venue">Machine Learning and Knowledge Extraction Journal</p>
                <div class="links">
                    <a href="https://www.mdpi.com/2504-4990/6/3/74" target="_blank">Paper</a>
                </div>
            </div>

            <div class="publication">
                <h3>Scoping Review on Image-Text Multimodal Machine Learning Models</h3>
                <p class="authors">M. Rashid, P. Rivas</p>
                <p class="venue">International Conference on Computational Science and Computational Intelligence (CSCI), 2023</p>
                <div class="links">
                    <a href="https://www.rivas.ai/pdfs/rashid2023scoping.pdf" target="_blank">Paper</a>
                </div>
            </div>

            <div class="publication">
                <h3>Autonomous deblurring images and information extraction from documents using CycleGAN and mask RCNN</h3>
                <p class="authors">O. Hoque, M. Rashid, T. Zawad</p>
                <p class="venue">23rd International Conference on Computer and Information Technology (ICCIT) (IEEE), 2020 </p>
                <div class="links">
                    <a href="https://www.researchgate.net/profile/K-M-Tawsik-Jawad/publication/351290151_Autonomous_Deblurring_Images_and_Information_Extraction_from_Documents_Using_CycleGAN_and_Mask_RCNN/links/628bcd913303d263c471e7d5/Autonomous-Deblurring-Images-and-Information-Extraction-from-Documents-Using-CycleGAN-and-Mask-RCNN.pdf" target="_blank">Paper</a>
                </div>
                
            </div>

            <div class="publication">
                <h3>Targeted face recognition and alarm generation for security surveillance using single shot multibox detector (SSD)</h3>
                <p class="authors">T Jawad, B Maisha, S Nazmus</p>
                <p class="venue">Int. J. Comput. Appl, 2019</p>
                <div class="links">
                    <a href="https://www.researchgate.net/profile/Nazmus-Sakib-71/publication/338013347_Targeted_Face_Recognition_and_Alarm_Generation_for_Security_Surveillance_using_Single_Shot_Multibox_Detector_SSD/links/61a836bdca2d401f27b93f98/Targeted-Face-Recognition-and-Alarm-Generation-for-Security-Surveillance-using-Single-Shot-Multibox-Detector-SSD.pdf" target="_blank">Paper</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects">
        <div class="container">
            <h2>Selected Projects</h2>
            
            <div class="project">
                <h3>Adversarial Robustness in Vision Language Models</h3>
                <p>Developed multimodal machine learning model for image captioning using ViT and GPT-2:</p>
                <ul>
                    <li>Performed adversarial training to identify vulnerable modalities in the architecture</li>
                    <li>Strategically froze less vulnerable components to focus training on weaker modalities</li>
                    <li>Increased model performance from 74% to 78% through targeted robustness improvements</li>
                </ul>
                <a href="#" class="project-link" target="_blank">â†’ View Project</a>
            </div>

            <div class="project">
                <h3>Analyzing Adversarial Vulnerability of Stable Diffusion</h3>
                <p>Comprehensive analysis of text-to-image generation model robustness:</p>
                <ul>
                    <li>Fine-tuned Stable Diffusion on COCO dataset for high-quality image generation</li>
                    <li>Applied gradient-based attacks (FGSM, PGD) to image latent components and text-based attacks to prompts</li>
                    <li>Measured robustness using LPIPS (perceptual change) and CLIP score (image-text alignment)</li>
                </ul>
                <a href="#" class="project-link" target="_blank">â†’ View Project</a>
            </div>

            <div class="project">
                <h3>Adversarial Sensitivity Analysis Metric for VLMs</h3>
                <p>Novel framework for evaluating vision-language model vulnerabilities:</p>
                <ul>
                    <li>Conducted controlled modality stress test on CLIP by alternately freezing ViT and text encoders</li>
                    <li>Discovered vision modality showed 9% larger accuracy drop than text modality on Hateful Memes dataset</li>
                    <li>Defined Adversarial Vulnerability Index (AVI) for progressive evaluation from random noise to adversarial attacks</li>
                </ul>
                <a href="#" class="project-link" target="_blank">â†’ View Project</a>
            </div>

            <div class="project">
                <h3>CyberPolice - Cyberbullying Detection System</h3>
                <p>Deep learning approach for detecting cyberbullying in social media:</p>
                <ul>
                    <li>Developed three models: LSTM, Bi-LSTM, and Bi-LSTM with Transformer for Facebook post analysis</li>
                    <li>Employed advanced sentiment analysis to identify emotional context in posts</li>
                    <li>Enhanced detection precision through innovative NLP and deep learning techniques</li>
                </ul>
                <a href="#" class="project-link" target="_blank">â†’ View Project</a>
            </div>

            <div class="project">
                <h3>Cyberbullying Detection using Machine Learning</h3>
                <p>Classical ML approach for social media content moderation:</p>
                <ul>
                    <li>Applied Logistic Regression, SVM, and KNN algorithms for detecting cyberbullying</li>
                    <li>Conducted comprehensive comparative analysis of supervised and unsupervised approaches</li>
                    <li>Effectively identified and classified abusive language in social media content</li>
                </ul>
                <a href="#" class="project-link" target="_blank">â†’ View Project</a>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>&copy; 2025 Maisha Binte Rashid. All rights reserved.</p>
        </div>
    </footer>

    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('nav a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });
    </script>
</body>
</html>








